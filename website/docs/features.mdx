---
id: features
title: Features
---

import useBaseUrl from '@docusaurus/useBaseUrl';

## Ridge Regression

In order to address multicollinearity among many regressors and prevent
overfitting we apply a regularization technique to reduce variance at the cost
of introducing some bias. This approach tends to improve the predictive
performance of MMMs. The most common regularization, and the one we are using in
this code is Ridge regression. The mathematical notation for Ridge regression
is:

<img alt="Ridge Regression Formula" src={useBaseUrl('img/Ridge.png')} />

If we go a bit deeper into the actual components we will be using within the model specification,
besides the lambda penalization term above, we can identify the following formula:

<img alt="Ridge Regression Formula" src={useBaseUrl('img/model_specification.png')} />

---

## Variable Transformations

There are two main variable transformations in the code:

1. Adstock
2. Diminishing returns

---

### Adstock

This technique is very useful for the better and more accurate representation of
the real carryover effect of marketing campaigns. Moreover, it helps to improve
the understanding of decay effects and how this can be used in campaign
planning. It reflects the theory that the effects of advertising can lag and
decay following initial exposure. In other words, not all effects of advertising
are felt immediately—memory builds and people sometimes delay action—and this
awareness diminishes as time passes.

There are two adstock techniques you may choose from the code:

1. Geometric: Traditionally the exponential decay function is used and
   controlled by theta, the decay parameter. For example, an ad-stock of theta =
   0.75 means that 75% of the impressions in Period 1 were brought to Period 2.
   Mathematically the traditional exponential adstock decay effect is defined
   as:

<img alt="Geometric Formula" src={useBaseUrl('img/geometric.png')} />



2. Weibull: Weibull survival function (Weibull distribution) provides much more
   flexibility in the shape and scale of the distribution. The formula is
   defined as: <img alt="Weibull Formula" src={useBaseUrl('img/weibull.png')} />

---

### Diminishing returns

The theory of diminishing returns holds that each additional unit of advertising
increases the response, but at a declining rate. This key marketing principle is
reflected in marketing mix models as a variable transformation.

<img alt="Diminishing returns1" src={useBaseUrl('img/diminishingreturns1.png')}
/>

The nonlinear response to a media variable on the dependent variable can be
modelled using a variety of functions. For example, we can use a simple
logarithm transformation (taking the log of the units of advertising log(x) ),
or a power transformation (x^alpha). In the case of a power transformation, the
modeler tests the different variables (different levels of parameter x) for the
highest significance of the variable in the model and the highest significance
of the equation overall. However, the most common approach is to use the
flexible S-curve transformation:

<img alt="Diminishing returns1" src={useBaseUrl('img/diminishingreturns2.png')}
/>

The variations of the parameters give modelers full flexibility on the look of
the S-curve, specifically the shape and the inflection points:

<img alt="Diminishing returns1" src={useBaseUrl('img/diminishingreturns3.png')}
/>

---

## Organic Media

Robyn enables users to specify `organic_vars` to model marketing activities without direct spend. Typically, this may include newsletter, push notification, social media posts, among other efforts. Moreover, organic variables are expected to have similar carryover (adstock) and saturating behavior as paid media variables. The respective transformation techniques such as geometric or Weibull transformation for adstock; or Hill transformation for saturation, are also applied to organic variables.

---

### Examples of typical organic variables
- Reach / Impressions on blog posts
- Impressions on organic & unpaid social media
- SEO improvements
- Email shots
- Reach on UGC

Below a chart showing the different types of organic variables that could be modeled

<img alt="organic media" src={useBaseUrl('/img/organic-media.png')} />

---

## Meta Prophet

### Automated trend, seasonality and holiday effect decomposition

---

**Prophet** has been included in the code in order to improve the fit and forecast of
time-series by decomposing the effect of trend, seasonality and holiday
components in the response variable (Sales, conversions, etc.).

**Prophet** is a Facebook original procedure for **forecasting time
series** data, based on a model where **non-linear trends** are fit with yearly,
weekly, and daily seasonality, plus holiday effects. It works best with time
series that have strong seasonal effects and several seasons of historical data.
More details can be found [here](https://facebook.github.io/prophet/docs/).

---

### Prophet decomposition plot example
Trend, season, holiday and an extra regressor ("events" in this case) decomposition by Prophet.
Weekday is not used because the sample data is weekly (not daily).
Robyn uses Prophet to also decompose categorical variables as an extra regressor which can simplify later programming.
For technical details of decomposition, please refer to Prophet's documentation [here](https://facebook.github.io/prophet/docs/trend_changepoints.html).

<img alt="prophet 2" src={useBaseUrl('/img/prophet2.png')} />

---

##  Nevergrad Automated Hyperparameter Selection Optimization

---

MMMs are likely to **contain high cardinality of parameters**. ie. alphas and gammas for the diminishing returns (Hill) function, as well as, thetas for geometric ad stock transformation.
In addition, parameters dimensionality increases proportionally with the total number of marketing channels to be measured. Thus, it is extremely necessary to deal with a high dimensionality parameter space where, the greater the number of parameters, **the greater the model complexity, its dimensionality and computational requirements.**

In order to achieve computational efficiency while optimizing overall model accuracy, we leverage [**Meta’s Nevergrad gradient-free optimization platform**](https://facebookresearch.github.io/nevergrad/).  Nevergrad allows us to optimize the explore and exploit balance through the **ask** and **tell** commands, in order to perform a multi-objective optimization tha balances out the Normalized Root Mean Square Error (**NRMSE**) and **decomp.RSSD** ratio (Relationship between spend share and channels coefficient decomposition share) providing a set of **Pareto optimal model solutions**

Please find below an example of a common chart for the Pareto model solutions.
Each dot in the chart represents an explored model solution, while the lower-left corner lines are Pareto-fronts 1-3 and contains the best possible model results from all iterations.
The two axes (NRMSE on x and DECOMP.RSSD on y) are the two objective functions to be minimized.
As the iteration increases, a trend down the lower-left corner of the coordinate can be clearly observed.
This is a proof of Nevergrad's ability to drive the model result towards an optimal direction.

<img alt="pareto chart" src={useBaseUrl('/img/pareto2.png')} />

The premise of an **evolutionary algorithm** is that of natural selection. In an evolutionary algorithm you may have a set of iterations where some combinations of coefficients that will be explored by the model will survive and proliferate, while unfit models will die off and not contribute to the gene pool of further generations, much like in natural selection.
In robyn, we recommend a minimum of 2000 iterations where each of these will provide feedback to its upcoming generation, and therefore guide the model towards the optimal coefficient values for alphas, gammas and thetas. We also recommend a minimum of 5 trials which are a set of independent initiations of the model that will each of them have the number of iterations you set under ‘set_iter’ object. E.g. 2000 iterations on set_iter x 5 trials = 10000 different iterations and possible model solutions.

---

## Calibration with Experiments

By applying results from randomized controlled-experiments, you may improve the
accuracy of your marketing mix models dramatically. It is recommended to run
these on a recurrent basis to keep the model calibrated permanently. In general,
we want to compare the experiment result with the MMM estimation of a marketing
channel. Conceptually, this method is like a Bayesian method, in which we use
experiment results as a prior to shrink the coefficients of media variables. A
good example of these types of experiments is Facebook’s conversion lift tool
which can help guide the model towards a specific range of incremental values.

<img alt="Calibration chart" src={useBaseUrl('/img/calibration1.png')} />

The figure illustrates the calibration process for one MMM candidate model.
[**Facebook’s Nevergrad gradient-free optimization platform**](https://facebookresearch.github.io/nevergrad/) allows us to include the **MAPE(cal,fb)** as a third optimization score besides Normalized Root Mean Square Error (**NRMSE**) and **decomp.RSSD** ratio (Please refer to the automated hyperparameter selection and optimization for further details) providing a set of **Pareto optimal model solutions** that minimize and converge to a set of Pareto optimal model candidates. This calibration method can be applied to other media channels which run experiments, the more channels that are calibrated, the more accurate the MMM model.

## Outputs & Diagnostics

The MMM code will automatically generate a set of plots under the folder you specify on the ‘model_output_collect’ object. Each of these plots represents one of the optimal model solutions as a result of the multi-objective optimization Pareto optimal process mentioned in the **‘Automated hyperparameter selection and optimization’** section. Please find below an example of the model output:

<img alt="ModelResults1 chart" src={useBaseUrl('/img/ModelResults1.png')} />

As you may observe we have 6 different charts above:
1. **Response decomposition waterfall by predictor:** This chart reflects the percentage of each of the variables effect (Baseline and Media variables + intercept) on the response variable. E.g. If season effect says it's 40.5% that means that 40.5% of the total sales can be attributed to seasonality.
2. **Share of spend vs. share of effect:** This plot reflects the comparison of the total effect each channel had by means of the decomposition of the coefficients into the response variable divided by the total effect. As well as, the total spend (cost or investment) each channel had and its relative share over total marketing spend. We also plot the return on investment (ROI) each channel had which can give you an idea over the most profitable channels.
3. **Average adstock decay rate:** This chart represents, on average, what is the percentage decay rate each channel had. The higher the decay rate, the longer the effect in time for that specific channel media exposure.
4. **Actual vs. predicted response:** This plot shows the actual data for the response variable E.g sales, and how the modeled predicted data for that response variable is capturing the real curve. We aim for models that can capture most of the variance from the actual data and therefore the R-squared is closer to 1 while NRMSE is low.
5. **Response curves and mean spend by channel:** These are the diminishing returns response curves from hill function. They represent how saturated a channel is and therefore, may suggest potential budget reallocation strategies. The faster the curves reach to an inflection point and to a horizontal/flat slope, the quicker they will saturate with each extra ($) spent.
6. **Fitted vs. residual:** This chart shows the relationship between fitted and residual values. A residual value is a measure of how much a regression line vertically misses a data point.  A residual plot is typically used to find problems with a regression. Some data sets are not good candidates for regression, such as points at widely varying distances from the line. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a nonlinear model is more appropriate.

Once you have analyzed the optimal model results plots and have chosen your model, you may introduce the model unique ID from the results in the previous section. E.g setting modID = "1_22_3" could be an example of a selected model from the list of best models in 'model_output_collect$allSolutions' results object. Once you run the budget allocator, results will be plotted and saved under the same folder where the model plots had been saved. The result would look like the following:

<img alt="budget allocator chart" src={useBaseUrl('/img/budgerAllocator1.png')} />

You may encounter three charts as in the example above:
1. **Initial vs. optimized budget allocation:** This channel shows the original spend share vs. the new optimized recommended one. If optimized share is greater than original, this would mean you will need to proportionally increase budgets for that channel according to the difference between both shares. And you would reduce budgets in the case spend share would be greater than optimized share.
2. **Initial vs. optimized mean response:** Similar to the chart above, we have initial and optimized shares, but this time over the total expected response E.g. Sales. The optimized response is the total increase in sales that we are expecting you to have if you switch budgets following the chart we explained above, increasing those with better share for optimized spend and decreasing spend for those with lower optimized spend than the initial.
3. **Response curve and mean spend by channel:** These again are the diminishing returns response curves from hill function. They represent how saturated a channel is and therefore, may suggest potential budget reallocation strategies. The faster the curves reach to an inflection point and to a horizontal/flat slope, the quicker they will saturate with each extra ($) spent. The initial mean spend is represented by a circle and the optimized one by the triangle.

---

## Modeling Window

---
 MMM models are built based on past data. Thus, one of the main requirements when building an MMM model is to be able to select the right dates from the historical data history that represent well both, the current business and the marketing investment situation. The advantage of a rolling window is that you may still use the full data history (E.g. 2 years) on your data for trend, seasonality and holiday effects to be modeled, while at the same time, defining a specific range (E.g. 6 months) for media, organic and contextual variables that better represents your current business and marketing scenario. You may specify a window_start and window_end dates within the robyn_inputs() function to define a modeling period as a subset of the total available data for media and other variables.

```
 InputCollect <- robyn_inputs(
...
   ,window_start = "2016-11-23"
   ,window_end = "2018-08-22"
...
 )
```
---

- **`window_start`** A character. Set the start date of the modelling period. The window start will determine the start date of the data period within your dataset you will be using to specifically regress the effects of media, organic and context variables on your dependent variable. We recommend using a full “dt_input” dataset with a minimum of 1 year of history, as it will be used in full for the model calculation of trend, seasonality and holidays effects. Whereas the window period will determine how much of the full data set will be used for media, organic and context variables. E.g. Uploading and using 2 years of data in “dt_input” but determining window_start and window_end as the last 6 months which reflect better the current business and/or marketing investment reality for budget decision making.
- **`window_end`** A character. Set the end date of the modelling period. Recommended to have a ratio of (independent variable : parameters) data points of 1:10. The window end will determine the end date of the data period within your dataset you will be using to specifically regress the effects of media, organic and context variables on your dependent variable. We recommend using a full “dt_input” dataset with a minimum of 1 year of history, as it will be used in full for the model calculation of trend, seasonality and holidays effects. Whereas the window period will determine how much of the full data set will be used for media, organic and context variables. E.g. Uploading and using 2 years of data in “dt_input” but determining window_start and window_end as the last 6 months which reflect better the current business and/or marketing investment reality for budget decision making.

---

You may look at the example below with most of 2017 and 2018 as the window that will be taken into consideration for media, organic and contextual variables modeling.

<img alt="pareto chart 2" src={useBaseUrl('/img/refresh-window.png')} />

---

## Continuous Reporting

---
Another strong capability requirement linked to the model window we identified, is the ability to refresh the model when new data would arrive. In other words, to be able to continuously report in a monthly, weekly or even daily frequency, based on a previously selected model but applied onto new fresh data. Consequently, enabling MMM to be a continuous reporting tool for actionable and timely decision-making that could feed your reporting or BI tools within the defined cadence.


The new `robyn_refresh()` function is able to continuously build and add new model periods, at any given cadence (weeks, months, etc.), based on previously selected models saved in the Robyn.RData object specified in the robyn_object.


For example, when updating the initial build with 4 weeks of new data, robyn_refresh() consumes the selected model of the initial build. What Robyn does is to set lower and upper bounds of hyperparameters for the new build which are consistent with the selected hyperparameters of the previous build. Therefore, stabilizing the effect of contextual and organic variables across old and new builds as well as, regulating the new share of effect of media variables towards the new added period spend level. Finally, returning aggregated results containing all previous builds for reporting purposes and their corresponding plots.

---
The example below shows the model refreshing mechanism for 5 different periods of time based in an initial window covering most of 2017 and 2018:

<img alt="pareto chart 2" src={useBaseUrl('/img/refresh-window.png')} />


---
You will also obtain a set of results for each refresh period that describes the assigned ROI and effects from each of the variables within the model. The baseline variable is the sum of all prophet variables (trend, seasonality, weekday and holiday) plus the intercept. The charts are based on simulated and do not have real-life implication:

<img alt="pareto chart 2" src={useBaseUrl('/img/refresh-reporting.png')} />


---
#### `robyn_refresh()` description
The `robyn_refresh()` function builds updated models based on the previously built models saved in the Robyn.RData object specified in robyn_object.
For example, when updating the initial build with 4 weeks of new data, `robyn_refresh()` consumes the selected model of the initial build.

What Robyn does, is to set lower and upper bounds of hyperparameters for the new build around the selected hyperparameters of the previous build,
stabilizes the effect of baseline variables across old and new builds and regulates the new effect share of media variables towards the latest spend level.
It returns aggregated results with all previous builds for reporting purposes and produces reporting plots.

---
#### `robyn_refresh()` usage
```
robyn_refresh(
  robyn_object,
  dt_input = dt_input,
  dt_holidays = dt_holidays,
  refresh_steps = 4,
  refresh_mode = "manual",
  refresh_iters = 1000,
  refresh_trials = 3,
  plot_pareto = TRUE
)
```
---
#### `robyn_refresh()` arguments

- **`robyn_object`**
A character. Path of the `Robyn.RData` object that contains all previous modeling information.
- **`dt_input`**
A data.frame. It should include all previous data and newly added data for the refresh.
- **`dt_holidays`**
A data.frame. Raw input holiday data. Load standard Prophet holidays using `data("dt_prophet_holidays")`.
- **`refresh_steps`**
An integer. It controls how many time units the refresh model build will move forward. For example, `refresh_steps = 4` on weekly data means the `InputCollect$window_start` & `InputCollect$window_end` will move forward 4 weeks.
- **`refresh_mode`**
A character. Options are "auto" and "manual". In auto mode, the `robyn_refresh()` function builds refresh models following given `refresh_steps` repeatedly until there is no more data available. In manual mode, the `robyn_refresh()` moves forward the `refresh_steps` only once.
- **`refresh_iters`**
An integer. The number of iterations per refresh. The rule of thumb is, the more new data added, the more iterations needed.
- **`refresh_trials`**
An integer. The number of trials per refresh. Defaults to 5 trials.
- **`plot_pareto`**
A logical value. Set it to FALSE to deactivate plotting and saving model onepagers, commonly used when testing models.

---

